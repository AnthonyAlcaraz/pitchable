---
phase: 02-knowledge-base
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - apps/api/src/knowledge-base/parsers/parser.interface.ts
  - apps/api/src/knowledge-base/parsers/pdf.parser.ts
  - apps/api/src/knowledge-base/parsers/docx.parser.ts
  - apps/api/src/knowledge-base/parsers/markdown.parser.ts
  - apps/api/src/knowledge-base/parsers/text.parser.ts
  - apps/api/src/knowledge-base/parsers/url.parser.ts
  - apps/api/src/knowledge-base/chunking/chunk.interface.ts
  - apps/api/src/knowledge-base/chunking/heading-chunker.ts
  - apps/api/src/knowledge-base/document-processing.processor.ts
  - apps/api/src/knowledge-base/knowledge-base.module.ts
autonomous: true

must_haves:
  truths:
    - "A PDF file upload results in extracted text stored as chunks in the database"
    - "A DOCX file upload results in extracted text stored as chunks in the database"
    - "A Markdown file upload results in extracted text with heading-aware chunks"
    - "A plain text file upload results in paragraph-based chunks"
    - "A URL source results in article content extracted and chunked"
    - "Document status transitions from UPLOADED to PARSING during text extraction"
    - "Document status transitions to ERROR with errorMessage on parser failure"
    - "Large sections are split into sub-chunks respecting maxChunkSize"
    - "Each chunk retains its parent heading context in metadata"
  artifacts:
    - path: "apps/api/src/knowledge-base/document-processing.processor.ts"
      provides: "BullMQ processor for document-processing queue"
      exports: ["DocumentProcessingProcessor"]
    - path: "apps/api/src/knowledge-base/chunking/heading-chunker.ts"
      provides: "Heading-aware semantic chunking with overlap"
      exports: ["chunkByHeadings"]
    - path: "apps/api/src/knowledge-base/parsers/pdf.parser.ts"
      provides: "PDF text extraction via pdf-parse"
      exports: ["PdfParser"]
  key_links:
    - from: "apps/api/src/knowledge-base/document-processing.processor.ts"
      to: "apps/api/src/knowledge-base/parsers/*.parser.ts"
      via: "method call based on mimeType/sourceType routing"
      pattern: "parse(Pdf|Docx|Markdown|Text|Url)"
    - from: "apps/api/src/knowledge-base/document-processing.processor.ts"
      to: "apps/api/src/knowledge-base/chunking/heading-chunker.ts"
      via: "function call"
      pattern: "chunkByHeadings"
    - from: "apps/api/src/knowledge-base/document-processing.processor.ts"
      to: "prisma.document.update"
      via: "status transitions"
      pattern: "status.*PARSING|EMBEDDING|READY|ERROR"
    - from: "apps/api/src/knowledge-base/knowledge-base.module.ts"
      to: "apps/api/src/knowledge-base/document-processing.processor.ts"
      via: "providers array"
      pattern: "DocumentProcessingProcessor"
---

<objective>
Build the document parsing pipeline and heading-aware semantic chunker. Creates 5 parser wrappers (PDF, DOCX, Markdown, plain text, URL) and a BullMQ processor that orchestrates: fetch from S3 -> parse -> chunk -> save chunks to DB (without embeddings -- that is Plan 03). Status transitions: UPLOADED -> PARSING -> (chunks saved without embeddings, status stays PARSING until Plan 03 processor runs).

Purpose: Converts raw uploaded files into structured text chunks. Without this, the embedding pipeline (Plan 03) has nothing to embed, and retrieval has no content to search.

Output: Working BullMQ document-processing processor that extracts text from all 5 source types and stores heading-aware chunks in the DocumentChunk table (without embedding vectors yet).
</objective>

<execution_context>
@C:\Users\33641\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\33641\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@C:\Users\33641\projects\slide-saas\.planning\PROJECT.md
@C:\Users\33641\projects\slide-saas\.planning\ROADMAP.md
@C:\Users\33641\projects\slide-saas\.planning\STATE.md
@C:\Users\33641\projects\slide-saas\.planning\phases\02-knowledge-base\02-RESEARCH.md
@C:\Users\33641\projects\slide-saas\.planning\phases\02-knowledge-base\02-01-SUMMARY.md

Key existing files to reference:
@C:\Users\33641\projects\slide-saas\apps\api\src\images\image-generation.processor.ts
@C:\Users\33641\projects\slide-saas\apps\api\src\knowledge-base\knowledge-base.module.ts
@C:\Users\33641\projects\slide-saas\apps\api\src\knowledge-base\knowledge-base.service.ts
@C:\Users\33641\projects\slide-saas\apps\api\src\knowledge-base\storage\s3.service.ts
@C:\Users\33641\projects\slide-saas\apps\api\src\prisma\prisma.service.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Parser interface and 5 parser implementations</name>
  <files>
    apps/api/src/knowledge-base/parsers/parser.interface.ts
    apps/api/src/knowledge-base/parsers/pdf.parser.ts
    apps/api/src/knowledge-base/parsers/docx.parser.ts
    apps/api/src/knowledge-base/parsers/markdown.parser.ts
    apps/api/src/knowledge-base/parsers/text.parser.ts
    apps/api/src/knowledge-base/parsers/url.parser.ts
  </files>
  <action>
**Step 1: Create parser interface** (`apps/api/src/knowledge-base/parsers/parser.interface.ts`):
```typescript
export interface ParseResult {
  text: string;
  title?: string;
  metadata?: Record<string, unknown>;
}

export interface DocumentParser {
  parse(input: Buffer | string): Promise<ParseResult>;
}
```

**Step 2: Create PDF parser** (`apps/api/src/knowledge-base/parsers/pdf.parser.ts`):
```typescript
import { Injectable, Logger } from '@nestjs/common';
import pdfParse from 'pdf-parse';
import type { DocumentParser, ParseResult } from './parser.interface.js';

@Injectable()
export class PdfParser implements DocumentParser {
  private readonly logger = new Logger(PdfParser.name);

  async parse(input: Buffer): Promise<ParseResult> {
    try {
      const data = await pdfParse(input);
      this.logger.log(`Parsed PDF: ${data.numpages} pages, ${data.text.length} chars`);
      return {
        text: data.text,
        title: data.info?.Title || undefined,
        metadata: {
          pageCount: data.numpages,
          author: data.info?.Author,
        },
      };
    } catch (error) {
      throw new Error(`PDF parsing failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}
```

NOTE on pdf-parse import: If `import pdfParse from 'pdf-parse'` fails at compile time, try `import * as pdfParse from 'pdf-parse'` and call it as `(pdfParse as any).default(input)` or just `pdfParse(input)`. The package uses `module.exports =` which can be tricky with ESM. Test at compile time and adjust.

**Step 3: Create DOCX parser** (`apps/api/src/knowledge-base/parsers/docx.parser.ts`):
```typescript
import { Injectable, Logger } from '@nestjs/common';
import mammoth from 'mammoth';
import type { DocumentParser, ParseResult } from './parser.interface.js';

@Injectable()
export class DocxParser implements DocumentParser {
  private readonly logger = new Logger(DocxParser.name);

  async parse(input: Buffer): Promise<ParseResult> {
    try {
      const result = await mammoth.extractRawText({ buffer: input });
      if (result.messages.length > 0) {
        this.logger.warn(`DOCX parse warnings: ${JSON.stringify(result.messages)}`);
      }
      this.logger.log(`Parsed DOCX: ${result.value.length} chars`);
      return { text: result.value };
    } catch (error) {
      throw new Error(`DOCX parsing failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}
```

NOTE on mammoth import: Same ESM consideration. If `import mammoth from 'mammoth'` fails, try `import * as mammoth from 'mammoth'`.

**Step 4: Create Markdown parser** (`apps/api/src/knowledge-base/parsers/markdown.parser.ts`):
```typescript
import { Injectable, Logger } from '@nestjs/common';
import { marked } from 'marked';
import type { DocumentParser, ParseResult } from './parser.interface.js';

@Injectable()
export class MarkdownParser implements DocumentParser {
  private readonly logger = new Logger(MarkdownParser.name);

  async parse(input: Buffer | string): Promise<ParseResult> {
    const text = typeof input === 'string' ? input : input.toString('utf-8');
    // For markdown, return the raw text as-is (the chunker handles heading detection)
    // We use the lexer to extract the title (first H1)
    try {
      const tokens = marked.lexer(text);
      const firstHeading = tokens.find(
        (t): t is { type: 'heading'; depth: number; text: string } =>
          t.type === 'heading' && (t as { depth: number }).depth === 1,
      );
      this.logger.log(`Parsed Markdown: ${text.length} chars`);
      return {
        text,
        title: firstHeading?.text,
      };
    } catch (error) {
      throw new Error(`Markdown parsing failed: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}
```

**Step 5: Create Text parser** (`apps/api/src/knowledge-base/parsers/text.parser.ts`):
```typescript
import { Injectable, Logger } from '@nestjs/common';
import type { DocumentParser, ParseResult } from './parser.interface.js';

@Injectable()
export class TextParser implements DocumentParser {
  private readonly logger = new Logger(TextParser.name);

  async parse(input: Buffer | string): Promise<ParseResult> {
    const text = typeof input === 'string' ? input : input.toString('utf-8');
    this.logger.log(`Parsed plain text: ${text.length} chars`);
    // Extract title from first non-empty line
    const firstLine = text.split('\n').find((l) => l.trim().length > 0)?.trim();
    return {
      text,
      title: firstLine && firstLine.length <= 100 ? firstLine : undefined,
    };
  }
}
```

**Step 6: Create URL parser** (`apps/api/src/knowledge-base/parsers/url.parser.ts`):
```typescript
import { Injectable, Logger } from '@nestjs/common';
import { Readability } from '@mozilla/readability';
import { JSDOM } from 'jsdom';
import type { DocumentParser, ParseResult } from './parser.interface.js';

@Injectable()
export class UrlParser implements DocumentParser {
  private readonly logger = new Logger(UrlParser.name);

  async parse(input: string): Promise<ParseResult> {
    const url = typeof input === 'string' ? input : input.toString();
    try {
      const response = await fetch(url, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (compatible; Pitchable/1.0)',
        },
        signal: AbortSignal.timeout(30000), // 30s timeout
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const html = await response.text();
      const dom = new JSDOM(html, { url });
      const reader = new Readability(dom.window.document);
      const article = reader.parse();

      if (!article) {
        throw new Error(`Could not extract article content from ${url}`);
      }

      this.logger.log(`Parsed URL "${url}": title="${article.title}", ${article.textContent.length} chars`);
      return {
        text: article.textContent,
        title: article.title,
        metadata: {
          siteName: article.siteName,
          excerpt: article.excerpt,
          sourceUrl: url,
        },
      };
    } catch (error) {
      throw new Error(`URL parsing failed for ${url}: ${error instanceof Error ? error.message : String(error)}`);
    }
  }
}
```
  </action>
  <verify>
Run `cd C:/Users/33641/projects/slide-saas/apps/api && npx tsc --noEmit` -- should compile.
Verify all 6 files exist in `apps/api/src/knowledge-base/parsers/`.
  </verify>
  <done>
5 parser classes (PdfParser, DocxParser, MarkdownParser, TextParser, UrlParser) plus the shared interface. Each wraps a third-party library with error handling and logging. All are @Injectable() NestJS services.
  </done>
</task>

<task type="auto">
  <name>Task 2: Heading-aware chunker and BullMQ document-processing processor</name>
  <files>
    apps/api/src/knowledge-base/chunking/chunk.interface.ts
    apps/api/src/knowledge-base/chunking/heading-chunker.ts
    apps/api/src/knowledge-base/document-processing.processor.ts
    apps/api/src/knowledge-base/knowledge-base.module.ts
  </files>
  <action>
**Step 1: Create chunk interface** (`apps/api/src/knowledge-base/chunking/chunk.interface.ts`):
```typescript
export interface DocumentChunkData {
  content: string;
  heading: string | null;
  headingLevel: number;
  chunkIndex: number;
  metadata: {
    sourceFile?: string;
    pageNumber?: number;
    sectionPath: string[];
  };
}
```

**Step 2: Create heading-aware chunker** (`apps/api/src/knowledge-base/chunking/heading-chunker.ts`):

This is a pure function (~100 lines). Key behavior:
- Split text on heading patterns (`# H1`, `## H2`, etc.) and blank-line paragraphs
- Each section = heading + body paragraphs
- If section > maxChunkSize chars: sub-split on paragraph boundaries, keep heading prefix
- If section < minChunkSize chars: merge with next section
- Add overlap: last overlapSize chars of previous chunk prepended to current
- Track heading hierarchy for sectionPath metadata

```typescript
import type { DocumentChunkData } from './chunk.interface.js';

export interface ChunkOptions {
  maxChunkSize?: number;   // ~2000 chars (~512 tokens)
  minChunkSize?: number;   // ~200 chars (~50 tokens)
  overlapSize?: number;    // ~200 chars (~50 tokens)
}

const DEFAULTS: Required<ChunkOptions> = {
  maxChunkSize: 2000,
  minChunkSize: 200,
  overlapSize: 200,
};

const HEADING_REGEX = /^(#{1,6})\s+(.+)$/;

interface Section {
  heading: string | null;
  headingLevel: number;
  body: string;
}

export function chunkByHeadings(
  text: string,
  options: ChunkOptions = {},
): DocumentChunkData[] {
  const opts = { ...DEFAULTS, ...options };
  const lines = text.split('\n');

  // 1. Split into sections by headings
  const sections: Section[] = [];
  let currentSection: Section = { heading: null, headingLevel: 0, body: '' };

  for (const line of lines) {
    const match = line.match(HEADING_REGEX);
    if (match) {
      // Push previous section if it has content
      if (currentSection.body.trim().length > 0 || currentSection.heading) {
        sections.push(currentSection);
      }
      currentSection = {
        heading: match[2].trim(),
        headingLevel: match[1].length,
        body: '',
      };
    } else {
      currentSection.body += line + '\n';
    }
  }
  // Push final section
  if (currentSection.body.trim().length > 0 || currentSection.heading) {
    sections.push(currentSection);
  }

  // 2. If no sections found (no headings), create one section from entire text
  if (sections.length === 0) {
    sections.push({ heading: null, headingLevel: 0, body: text });
  }

  // 3. Merge small sections
  const merged: Section[] = [];
  for (const section of sections) {
    const contentLength = (section.heading || '').length + section.body.trim().length;
    if (
      merged.length > 0 &&
      contentLength < opts.minChunkSize &&
      !section.heading // Only merge if current section has no heading
    ) {
      merged[merged.length - 1].body += '\n' + section.body;
    } else {
      merged.push({ ...section });
    }
  }

  // 4. Split oversized sections and build chunks
  const chunks: DocumentChunkData[] = [];
  const headingStack: string[] = [];
  let chunkIndex = 0;

  for (const section of merged) {
    // Track heading hierarchy
    if (section.heading) {
      // Pop headings at same or deeper level
      while (headingStack.length >= section.headingLevel) {
        headingStack.pop();
      }
      headingStack.push(section.heading);
    }

    const sectionPath = [...headingStack];
    const headingPrefix = section.heading ? `${section.heading}\n\n` : '';
    const bodyText = section.body.trim();
    const fullContent = headingPrefix + bodyText;

    if (fullContent.length <= opts.maxChunkSize) {
      // Section fits in one chunk
      if (fullContent.trim().length >= opts.minChunkSize || chunks.length === 0) {
        chunks.push({
          content: addOverlap(chunks, fullContent.trim(), opts.overlapSize),
          heading: section.heading,
          headingLevel: section.headingLevel,
          chunkIndex: chunkIndex++,
          metadata: { sectionPath },
        });
      } else if (chunks.length > 0) {
        // Too small -- append to previous chunk
        chunks[chunks.length - 1].content += '\n\n' + fullContent.trim();
      }
    } else {
      // Split on paragraph boundaries (double newline)
      const paragraphs = bodyText.split(/\n\s*\n/).filter((p) => p.trim().length > 0);
      let buffer = headingPrefix;

      for (const paragraph of paragraphs) {
        if (buffer.length + paragraph.length > opts.maxChunkSize && buffer.trim().length > 0) {
          chunks.push({
            content: addOverlap(chunks, buffer.trim(), opts.overlapSize),
            heading: section.heading,
            headingLevel: section.headingLevel,
            chunkIndex: chunkIndex++,
            metadata: { sectionPath },
          });
          buffer = headingPrefix + paragraph + '\n\n';
        } else {
          buffer += paragraph + '\n\n';
        }
      }

      // Flush remaining buffer
      if (buffer.trim().length > 0) {
        chunks.push({
          content: addOverlap(chunks, buffer.trim(), opts.overlapSize),
          heading: section.heading,
          headingLevel: section.headingLevel,
          chunkIndex: chunkIndex++,
          metadata: { sectionPath },
        });
      }
    }
  }

  return chunks;
}

function addOverlap(
  existingChunks: DocumentChunkData[],
  content: string,
  overlapSize: number,
): string {
  if (existingChunks.length === 0 || overlapSize <= 0) return content;
  const prevContent = existingChunks[existingChunks.length - 1].content;
  const overlap = prevContent.slice(-overlapSize);
  // Only add overlap if it doesn't duplicate the start of the current content
  if (content.startsWith(overlap)) return content;
  return overlap + '\n\n' + content;
}
```

**Step 3: Create BullMQ document-processing processor** (`apps/api/src/knowledge-base/document-processing.processor.ts`):

Follow the exact pattern from `image-generation.processor.ts` (extends WorkerHost, @Processor decorator):

```typescript
import { Processor, WorkerHost } from '@nestjs/bullmq';
import { Logger } from '@nestjs/common';
import { Job } from 'bullmq';
import { PrismaService } from '../prisma/prisma.service.js';
import { S3Service } from './storage/s3.service.js';
import { PdfParser } from './parsers/pdf.parser.js';
import { DocxParser } from './parsers/docx.parser.js';
import { MarkdownParser } from './parsers/markdown.parser.js';
import { TextParser } from './parsers/text.parser.js';
import { UrlParser } from './parsers/url.parser.js';
import { chunkByHeadings } from './chunking/heading-chunker.js';
import { DocumentStatus } from '../../generated/prisma/enums.js';
import type { DocumentProcessingJobData } from './knowledge-base.service.js';

@Processor('document-processing')
export class DocumentProcessingProcessor extends WorkerHost {
  private readonly logger = new Logger(DocumentProcessingProcessor.name);

  constructor(
    private readonly prisma: PrismaService,
    private readonly s3: S3Service,
    private readonly pdfParser: PdfParser,
    private readonly docxParser: DocxParser,
    private readonly markdownParser: MarkdownParser,
    private readonly textParser: TextParser,
    private readonly urlParser: UrlParser,
  ) {
    super();
  }

  async process(job: Job<DocumentProcessingJobData>): Promise<void> {
    const { documentId, sourceType, mimeType, s3Key, rawText, url } = job.data;
    this.logger.log(`Processing document ${documentId} (type: ${sourceType})`);

    try {
      // 1. Update status to PARSING
      await this.prisma.document.update({
        where: { id: documentId },
        data: { status: DocumentStatus.PARSING },
      });

      // 2. Extract text based on source type
      let extractedText: string;

      if (sourceType === 'TEXT') {
        const result = await this.textParser.parse(rawText!);
        extractedText = result.text;
      } else if (sourceType === 'URL') {
        const result = await this.urlParser.parse(url!);
        extractedText = result.text;
        // Update title if parser found one and current title is the URL
        if (result.title) {
          await this.prisma.document.update({
            where: { id: documentId },
            data: { title: result.title },
          });
        }
      } else {
        // FILE type -- download from S3 and parse by MIME type
        const buffer = await this.s3.getBuffer(s3Key!);
        const result = await this.parseByMimeType(buffer, mimeType!);
        extractedText = result.text;
      }

      if (!extractedText || extractedText.trim().length === 0) {
        throw new Error('No text could be extracted from the document');
      }

      // 3. Chunk the extracted text
      const chunks = chunkByHeadings(extractedText, {
        maxChunkSize: 2000,   // ~512 tokens
        minChunkSize: 200,    // ~50 tokens
        overlapSize: 200,     // ~50 tokens overlap
      });

      this.logger.log(`Document ${documentId}: extracted ${extractedText.length} chars, ${chunks.length} chunks`);

      // 4. Store chunks in DB (without embeddings -- Plan 03 handles that)
      // Use a transaction to insert all chunks atomically
      await this.prisma.$transaction(
        chunks.map((chunk) =>
          this.prisma.documentChunk.create({
            data: {
              documentId,
              content: chunk.content,
              heading: chunk.heading,
              headingLevel: chunk.headingLevel,
              chunkIndex: chunk.chunkIndex,
              metadata: chunk.metadata,
            },
          }),
        ),
      );

      // 5. Update document status to EMBEDDING (ready for Plan 03's embedding step)
      // NOTE: If OPENAI_API_KEY is not set, the embedding processor (Plan 03) will
      // be the one to move status to READY or ERROR. We set EMBEDDING here to signal
      // "chunks are stored, waiting for embeddings."
      await this.prisma.document.update({
        where: { id: documentId },
        data: {
          status: DocumentStatus.EMBEDDING,
          chunkCount: chunks.length,
        },
      });

      this.logger.log(`Document ${documentId}: ${chunks.length} chunks stored, status=EMBEDDING`);
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      this.logger.error(`Document ${documentId} processing failed: ${errorMessage}`);

      await this.prisma.document.update({
        where: { id: documentId },
        data: {
          status: DocumentStatus.ERROR,
          errorMessage,
        },
      });

      throw error; // Re-throw so BullMQ marks job as failed
    }
  }

  private async parseByMimeType(buffer: Buffer, mimeType: string) {
    if (mimeType === 'application/pdf') {
      return this.pdfParser.parse(buffer);
    }
    if (
      mimeType ===
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
    ) {
      return this.docxParser.parse(buffer);
    }
    if (mimeType === 'text/markdown' || mimeType === 'text/x-markdown') {
      return this.markdownParser.parse(buffer);
    }
    if (mimeType === 'text/plain') {
      return this.textParser.parse(buffer);
    }
    // Fallback: try as plain text
    this.logger.warn(`Unknown MIME type "${mimeType}", treating as plain text`);
    return this.textParser.parse(buffer);
  }
}
```

**Step 4: Update knowledge-base.module.ts** to add the processor and all parsers to providers.

Read the current `knowledge-base.module.ts` (created in Plan 01) and add these imports and providers:

Add imports at top:
```typescript
import { DocumentProcessingProcessor } from './document-processing.processor.js';
import { PdfParser } from './parsers/pdf.parser.js';
import { DocxParser } from './parsers/docx.parser.js';
import { MarkdownParser } from './parsers/markdown.parser.js';
import { TextParser } from './parsers/text.parser.js';
import { UrlParser } from './parsers/url.parser.js';
```

Add to providers array (after `S3Service`):
```typescript
DocumentProcessingProcessor,
PdfParser,
DocxParser,
MarkdownParser,
TextParser,
UrlParser,
```

The full providers array should be:
```typescript
providers: [
  KnowledgeBaseService,
  S3Service,
  DocumentProcessingProcessor,
  PdfParser,
  DocxParser,
  MarkdownParser,
  TextParser,
  UrlParser,
],
```
  </action>
  <verify>
Run `cd C:/Users/33641/projects/slide-saas/apps/api && npx tsc --noEmit` -- should compile.
Verify `parsers/` directory has 6 files (interface + 5 parsers).
Verify `chunking/` directory has 2 files (interface + chunker).
Verify `document-processing.processor.ts` exists.
Verify `knowledge-base.module.ts` providers include all parsers and the processor.
  </verify>
  <done>
- 5 parsers (PDF via pdf-parse, DOCX via mammoth, Markdown via marked, plain text, URL via @mozilla/readability + jsdom)
- Heading-aware semantic chunker: splits on headings, respects max/min chunk size, adds overlap, tracks heading hierarchy
- BullMQ document-processing processor: fetch -> parse -> chunk -> store chunks in DB
- Status transitions: UPLOADED -> PARSING -> EMBEDDING (waiting for embeddings)
- Error handling: status -> ERROR with errorMessage on any failure
- All parsers and processor registered in KnowledgeBaseModule
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. `parsers/` has 6 files, `chunking/` has 2 files, processor exists
3. Module providers include all parsers and the processor
4. Processor follows the exact same pattern as ImageGenerationProcessor (extends WorkerHost, @Processor decorator, try/catch with status update on error)
5. Chunker handles: headings, no headings (paragraph fallback), oversized sections, undersized sections, overlap
</verification>

<success_criteria>
- All 5 source types (PDF, DOCX, MD, TXT, URL) can be parsed to plain text
- Heading-aware chunker produces chunks with heading context, respects size limits
- BullMQ processor orchestrates the full parse-chunk pipeline
- Status transitions work: UPLOADED -> PARSING -> EMBEDDING (or ERROR)
- Chunks stored in DocumentChunk table with heading, headingLevel, chunkIndex, metadata
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-knowledge-base/02-02-SUMMARY.md`
</output>
